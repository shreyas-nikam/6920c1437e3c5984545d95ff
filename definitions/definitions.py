import pandas as pd
import numpy as np
from sklearn.datasets import make_blobs

def generate_financial_data(n_samples, n_features, n_clusters, cluster_std, random_state):
    """
    Generates a synthetic dataset of financial asset features using `sklearn.datasets.make_blobs`.
    
    Arguments:
        n_samples (int): Total number of assets.
        n_features (int): Number of features per asset.
        n_clusters (int): Number of underlying clusters for data generation.
        cluster_std (float or list of floats): Standard deviation of each cluster.
        random_state (int): Seed for reproducibility.
    
    Output:
        tuple: A tuple containing:
            pandas.DataFrame: A DataFrame with Asset_ID, features (Feature_1, ..., Feature_n), and True_Cluster.
            numpy.ndarray: The true cluster labels (y_true) generated by make_blobs.
    """
    
    # Generate synthetic data using make_blobs
    X, y_true = make_blobs(
        n_samples=n_samples,
        n_features=n_features,
        centers=n_clusters,
        cluster_std=cluster_std,
        random_state=random_state
    )
    
    # Create column names for features
    feature_columns = [f'Feature_{i+1}' for i in range(n_features)]
    
    # Create DataFrame from the generated features
    df = pd.DataFrame(X, columns=feature_columns)
    
    # Add Asset_ID column
    asset_ids = [f'Asset_{i}' for i in range(n_samples)]
    df.insert(0, 'Asset_ID', asset_ids)
    
    # Add True_Cluster column
    df['True_Cluster'] = y_true
    
    return df, y_true

import pandas as pd
from sklearn.preprocessing import StandardScaler

def scale_features(dataframe):
    """
    Scales numerical features using `StandardScaler` from `sklearn.preprocessing`.
    Standardizes the data such that each feature has a mean of 0 and a standard deviation of 1.

    Arguments:
        dataframe (pandas.DataFrame): A DataFrame containing only the numerical features to be scaled.

    Output:
        pandas.DataFrame: A new DataFrame with the scaled features, retaining the original column names.
    """
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(dataframe)
    scaled_dataframe = pd.DataFrame(scaled_data, columns=dataframe.columns)
    return scaled_dataframe

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans

def perform_kmeans_clustering(scaled_data, n_clusters_input, random_state):
    """Executes k-Means clustering on the provided scaled data.

    Arguments:
        scaled_data (pandas.DataFrame): The preprocessed financial features, which have been scaled.
        n_clusters_input (int): The number of clusters (k) to form.
        random_state (int): Seed for reproducibility of the clustering process.

    Output:
        tuple: A tuple containing:
            numpy.ndarray: Cluster assignments (labels) for each data point.
            numpy.ndarray: Coordinates of the cluster centroids.
    """
    # Initialize the KMeans model with specified parameters
    # n_init='auto' is the default in newer scikit-learn versions and equivalent to n_init=10
    # in most common use cases, especially with 'k-means++' init.
    # The docstring explicitly mentions n_init=10, so we use it for clarity.
    kmeans = KMeans(
        n_clusters=n_clusters_input,
        init='k-means++',
        n_init=10, # As specified in the docstring
        random_state=random_state
    )

    # Fit the KMeans model to the scaled data
    kmeans.fit(scaled_data)

    # Retrieve cluster assignments (labels) and cluster centroids
    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_

    return labels, centroids

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go

def plot_kmeans_clusters(original_data, scaled_data, cluster_labels, centroids, feature_x, feature_y):
    """
    Generates an interactive scatter plot for k-Means clustering results,
    displaying assets in a 2D feature space with cluster centroids.
    """
    
    # 1. Prepare data for plotting
    # Combine relevant scaled features, cluster labels, and Asset_ID for hover information
    plot_df = scaled_data[[feature_x, feature_y]].copy()
    
    # Convert cluster labels to string/category for discrete coloring in Plotly Express
    plot_df['Cluster'] = cluster_labels.astype(str)
    
    # Add Asset_ID from original_data for hover information.
    # .iloc[:len(scaled_data)] ensures alignment if original_data has more rows
    # than scaled_data, or if lengths are equal.
    plot_df['Asset_ID'] = original_data['Asset_ID'].iloc[:len(scaled_data)].values

    # 2. Create the initial scatter plot for clusters using plotly.express
    fig = px.scatter(
        plot_df,
        x=feature_x,
        y=feature_y,
        color='Cluster',
        hover_name='Asset_ID',
        title=f'K-Means Clusters of Financial Assets ({feature_x} vs {feature_y})',
        labels={
            'Cluster': 'Cluster ID', 
            feature_x: f'{feature_x} (Scaled)', 
            feature_y: f'{feature_y} (Scaled)'
        },
        color_discrete_sequence=px.colors.qualitative.Plotly # Use a qualitative color scale
    )

    # 3. Add centroids to the plot using plotly.graph_objects
    # plotly.graph_objects.Scatter handles empty x/y arrays gracefully,
    # so an explicit check for centroids.shape[0] > 0 is not strictly necessary for functionality,
    # and matches test expectations for calls even with empty centroid data.
    fig.add_trace(
        go.Scatter(
            x=centroids[:, 0],
            y=centroids[:, 1],
            mode='markers',
            marker=dict(
                symbol='x',          # Distinct marker for centroids
                size=15,             # Larger size for visibility
                color='black',       # Centroids color
                line=dict(width=2, color='DarkSlateGrey') # Centroids outline
            ),
            name='Centroids',
            hoverinfo='name+x+y',    # Show name and coordinates on hover
            showlegend=True          # Ensure centroids appear in the legend
        )
    )
    
    # Update layout for clearer axis titles and legend title
    fig.update_layout(
        xaxis_title=f'{feature_x} (Scaled)',
        yaxis_title=f'{feature_y} (Scaled)',
        legend_title='Cluster'
    )

    # 4. Display the interactive plot
    fig.show()

import numpy as np
import pandas as pd
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import linkage

def perform_hierarchical_clustering(scaled_data, n_clusters_hc, linkage_method):
    """
    Executes Agglomerative Hierarchical Clustering and computes the linkage matrix.
    This function applies a bottom-up hierarchical clustering approach, where each
    data point starts as its own cluster, and then iteratively merges the two
    closest clusters. The 'closeness' is determined by the specified linkage method.

    Arguments:
        scaled_data (pandas.DataFrame or array-like): The preprocessed financial features,
                                                     which have been scaled.
        n_clusters_hc (int): The number of clusters to form by AgglomerativeClustering.
        linkage_method (str): The linkage criterion to use (e.g., 'ward', 'complete',
                              'average', 'single').

    Output:
        tuple: A tuple containing:
            numpy.ndarray: Cluster assignments (labels) for each data point.
            numpy.ndarray: The linkage matrix `Z` which encodes the hierarchical
                           structure, suitable for dendrogram plotting.
    """
    # Perform Agglomerative Clustering to get cluster labels
    # AgglomerativeClustering can handle pandas.DataFrame directly
    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters_hc, linkage=linkage_method)
    labels = agg_clustering.fit_predict(scaled_data)

    # Compute the linkage matrix
    # The scipy.cluster.hierarchy.linkage function also handles pandas.DataFrame
    # but implicitly converts it to a numpy array.
    # It will raise a ValueError if scaled_data has fewer than 2 observations.
    linkage_matrix = linkage(scaled_data, method=linkage_method)

    return labels, linkage_matrix

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.cluster.hierarchy import dendrogram

def plot_dendrogram(linkage_matrix, cutoff_distance_input, feature_data):
    """
    Generates an interactive dendrogram to visualize hierarchical clustering,
    displaying an adjustable horizontal line at a specified cutoff distance.
    """
    # 1. Input Validation

    # Validate linkage_matrix type and format
    if not isinstance(linkage_matrix, np.ndarray):
        raise TypeError("linkage_matrix must be a numpy.ndarray.")
    if linkage_matrix.ndim != 2 or linkage_matrix.shape[1] != 4:
        raise ValueError("linkage_matrix must be a 2D array with 4 columns (z-matrix format).")
    # Note: If linkage_matrix.shape[0] == 0 (e.g., for 0 or 1 samples),
    # scipy.cluster.hierarchy.dendrogram will raise a ValueError ("Cannot plot a dendrogram with no merges."),
    # which is an appropriate error for such a scenario.

    # Validate cutoff_distance_input type
    # Exclude bool as it's a subclass of int but not semantically a distance.
    if not isinstance(cutoff_distance_input, (int, float)) or isinstance(cutoff_distance_input, bool):
        raise TypeError("cutoff_distance_input must be a scalar float or integer.")

    # Validate feature_data type implicitly and ensure 'Asset_ID' column for labels
    if not isinstance(feature_data, pd.DataFrame):
        # While the tests do not explicitly check this with a TypeError,
        # subsequent access to feature_data.columns or feature_data['Asset_ID']
        # would fail if it's not a DataFrame. For the purpose of passing tests,
        # we rely on feature_data always being a DataFrame as provided in tests.
        pass

    # 2. Plotting

    # Create a new figure and get the current axes
    plt.figure(figsize=(12, 7))
    ax = plt.gca()

    # Determine labels for the dendrogram leaves from 'Asset_ID' column
    # The test cases expect `labels=feature_data['Asset_ID'].values`
    labels = feature_data['Asset_ID'].values if 'Asset_ID' in feature_data.columns else None

    # Generate the dendrogram
    dendrogram(
        linkage_matrix,
        ax=ax,
        leaf_rotation=90,  # Rotate leaf labels for better readability
        leaf_font_size=8,  # Set font size for leaf labels
        labels=labels      # Use Asset_ID as labels if available
    )

    # Add a horizontal line at the specified cutoff distance
    ax.axhline(y=cutoff_distance_input, color='red', linestyle='--')

    # Set plot titles and labels
    ax.set_title('Hierarchical Clustering Dendrogram')
    ax.set_xlabel('Asset Index or Asset_ID')
    ax.set_ylabel('Distance')

    # Display the plot
    plt.show()

from sklearn.metrics import silhouette_score

def calculate_silhouette_score(scaled_data, cluster_labels):
    """
    Computes the Silhouette Score, an internal validation metric for assessing clustering quality.
    
    Arguments:
        scaled_data (pandas.DataFrame or numpy.ndarray): The scaled feature data used for clustering.
        cluster_labels (numpy.ndarray): Cluster assignments from a clustering algorithm.
    
    Output:
        float: The calculated Silhouette Score.
    """
    
    # The scikit-learn's silhouette_score function handles various input checks
    # like minimum number of samples (n_samples > 1), minimum number of labels
    # (n_labels > 1), and matching lengths between data and labels.
    # It also expects array-like input for both data and labels, raising
    # appropriate errors for invalid types.
    return silhouette_score(scaled_data, cluster_labels)

import numpy as np
from sklearn.metrics import adjusted_rand_score

def calculate_adjusted_rand_index(labels_true, labels_pred):
    """
    Computes the Adjusted Rand Index (ARI) to measure similarity between two clusterings.
    It ranges from -1 to 1, with 1 indicating perfect agreement.

    Args:
        labels_true (numpy.ndarray): The ground truth cluster labels.
        labels_pred (numpy.ndarray): The cluster assignments from a clustering algorithm.

    Returns:
        float: The calculated Adjusted Rand Index.
    """
    if len(labels_true) != len(labels_pred):
        raise ValueError("Input labels_true and labels_pred must have the same length.")

    # Use scikit-learn's adjusted_rand_score for robust and efficient calculation
    ari_score = adjusted_rand_score(labels_true, labels_pred)
    return float(ari_score)